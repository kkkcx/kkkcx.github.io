
# üìù Publications 
-----
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div>
<img src='images/papers/iccv-25-imu.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
### Egocentric Inertial Localization with Vision-Language Informed Action Cues
<p style="line-height:1.0">
<font size="2">
Mingfang Zhang, Ryo Yonetani, Yifei Huang, Liangyang Ouyang, <strong>Ruicong Liu</strong>, Yoichi Sato<br />
IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2025 <br /> 
<a href="https://arxiv.org/pdf/2505.14346">Paper</a> | 
<a href="https://github.com/mf-zhang/Ego-Inertial-Localization">Code</a>
<br />
</font>
</p>
</div>
</div>
<br />

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV</div>
<img src='images/papers/ijcv-24-jitter.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
### From Gaze Jitter to Domain Adaptation: Generalizing Gaze Estimation by Manipulating High-Frequency Components
<p style="line-height:1.0">
<font size="2">
<strong>Ruicong Liu</strong>, Haofei Wang, Feng Lu <br />
International Journal of Computer Vision (<strong>IJCV</strong>), 2024 <br /> 
<a href="https://link.springer.com/article/10.1007/s11263-024-02233-1">Paper</a> 
<br />
</font>
</p>
</div>
</div>
<br />

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div>
<img src='images/papers/eccv-24-actionvos.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
### ActionVOS: Actions as Prompts for Video Object Segmentation
<p style="line-height:1.0">
<font size="2">
Liangyang Ouyang, <strong>Ruicong Liu</strong>, Yifei Huang, Ryosuke Furuta, Yoichi Sato <br />
European Conference on Computer Vision (<strong>ECCV</strong>), 2024   <font color="red"><b>Oral, top 2%</b></font><br />
<a href="https://arxiv.org/pdf/2407.07402">Paper</a> | 
<a href="https://www.youtube.com/watch?v=dt-zDQKzq1I">Video</a> | 
<a href="https://github.com/ut-vision/ActionVOS">Code</a>
<br />
</font>
</p>
</div>
</div>
<br />

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div>
<img src='images/papers/eccv-24-mae.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
### Masked Video and Body-worn IMU Autoencoder for Egocentric Action Recognition
<p style="line-height:1.0">
<font size="2">
Mingfang Zhang, Yifei Huang, <strong>Ruicong Liu</strong>, Yoichi Sato <br />
European Conference on Computer Vision (<strong>ECCV</strong>), 2024 <br /> 
<a href="https://arxiv.org/pdf/2407.06628">Paper</a> |
<a href="https://www.youtube.com/watch?v=03vFGIf56vo">Video</a> |
<a href="https://github.com/mf-zhang/IMU-Video-MAE">Code</a>
<br />
</font>
</p>
</div>
</div>
<br />

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div>
<img src='images/papers/cvpr-24-s2dhand.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
### Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation
<p style="line-height:1.0">
<font size="2">
<strong>Ruicong Liu</strong>, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato <br />
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br /> 
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Single-to-Dual-View_Adaptation_for_Egocentric_3D_Hand_Pose_Estimation_CVPR_2024_paper.pdf">Paper</a> | 
<a href="https://www.youtube.com/watch?v=EzlmIre1PCY&t=25s">Video</a> |
<a href="https://github.com/ut-vision/S2DHand">Code</a>
<br />
</font>
</p>
</div>
</div>
<br />

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI</div>
<img src='images/papers/tpami-24-pnpga+.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
### PnP-GA+: Plug-and-Play Domain Adaptation for Gaze Estimation using Model Variants
<p style="line-height:1.0">
<font size="2">
<strong>Ruicong Liu</strong>, Yunfei Liu, Haofei Wang, Feng Lu <br />
IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2024 <br /> 
<a href="https://ieeexplore.ieee.org/abstract/document/10378867/">Paper</a>
<br />
</font>
</p>
</div>
</div>
<br />

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div>
<img src='images/papers/aaai-24-uvagaze.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
### UVAGaze: Unsupervised 1-to-2 Views Adaptation for Gaze Estimation
<p style="line-height:1.0">
<font size="2">
<strong>Ruicong Liu</strong>, Feng Lu <br />
AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2024 <br /> 
<a href="https://arxiv.org/pdf/2312.15644">Paper</a> | 
<a href="https://github.com/MickeyLLG/UVAGaze">Code</a>
<br />
</font>
</p>
</div>
</div>
<br />

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2021</div>
<img src='images/papers/iccv-21.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
### Generalizing gaze estimation with outlier-guided collaborative adaptation
<p style="line-height:1.0">
<font size="2">
Yunfei Liu*, <strong>Ruicong Liu*</strong>, Haofei Wang, Feng Lu <br />
IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br /> 
<a href="http://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Generalizing_Gaze_Estimation_With_Outlier-Guided_Collaborative_Adaptation_ICCV_2021_paper.pdf">Paper</a> | 
<a href="https://github.com/DreamtaleCore/PnP-GA">Code</a>
<br />
</font>
</p>
</div>
</div>
<br />

<!-- <div class='paper-box'><div class='paper-box-image'><div>
<img src='images/papers/arxiv-22-jitter.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
### Jitter does matter: Adapting gaze estimation to new domains
<p style="line-height:1.0">
<font size="2">
<strong>Ruicong Liu</strong>, Yiwei Bao, Mingjie Xu, Haofei Wang, Yunfei Liu, Feng Lu <br />
arXiv preprint arXiv:2210.02082 <br /> 
<a href="https://arxiv.org/pdf/2210.02082">Paper</a> 
<br />
</font>
</p>
</div>
</div>
<br /> -->

